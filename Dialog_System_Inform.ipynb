{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TLuckh/DialogActClassifier/blob/main/Dialog_System_Inform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py7zr"
      ],
      "metadata": {
        "id": "WgaEDE0Vuf2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WArxW88H6Wgn"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, json, os\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch, json, os\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import py7zr\n",
        "from spacy.lang.en import English"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIJayKIouTxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm54ZM9J5HuH",
        "outputId": "73e05754-679d-4787-e752-f944604c97d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RkC7F0P__rz"
      },
      "outputs": [],
      "source": [
        "home = './drive/MyDrive/Colab Notebooks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emsfjD2eader"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "\n",
        "def unzip(path_to_zip_file, directory_to_extract_to):\n",
        "  with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "      zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "\n",
        "unzip(home+'glove.6B.100d.zip', '')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and organizing the data\n",
        "\n",
        "@dataclass\n",
        "class Sample():\n",
        "    label:  dict\n",
        "    log:    dict\n",
        "\n",
        "\n",
        "def load_data(seven_zip_archive_path):\n",
        "    \"\"\"\n",
        "    Reads in the given 7zip-Archive, which is a reformat of the train & test-folders.\n",
        "    Returns:\n",
        "        train:  A list of Sample-Classes making up the training set\n",
        "        test:   A list of Sample-Classes making up the test set\n",
        "        ontology:  A list of possible slot-types and for each its possible values\n",
        "    \"\"\"\n",
        "\n",
        "    # Load zip and read:\n",
        "    tmp_file = py7zr.SevenZipFile(seven_zip_archive_path)\n",
        "    x = tmp_file.readall()\n",
        "    tmp_file.close()\n",
        "\n",
        "    #Filter to only obtain .json-Files:\n",
        "    irrelevant_files = [x for x in list(x.keys()) if \"json\" not in x]\n",
        "    relevant_files = [x for x in list(x.keys()) if \"json\" in x]\n",
        "    jsons_dict = {t:json.load(x[t]) for t in relevant_files}\n",
        "\n",
        "    #Remove train/scripts/config/ontology_dstc2.json:\n",
        "    assert 'train/scripts/config/ontology_dstc2.json' in relevant_files\n",
        "    relevant_files_new = [x for x in relevant_files if not x == \"train/scripts/config/ontology_dstc2.json\"]\n",
        "    ontology  = [x for x in relevant_files if x == \"train/scripts/config/ontology_dstc2.json\"][0]\n",
        "    relevant_files = relevant_files_new\n",
        "\n",
        "    #Split in test & train:\n",
        "    relevant_training_files = [x for x in relevant_files if 'train' in x]\n",
        "    relevant_test_files     = [x for x in relevant_files if 'test' in x]\n",
        "\n",
        "    #Get for each sample its folder containing log.json and label.json:\n",
        "    sample_folders_train = set()\n",
        "    sample_folders_test = set()\n",
        "    for file_name in relevant_training_files:\n",
        "        prefix = re.findall(\"^(([^/]+/){4})\", file_name)[0]\n",
        "        sample_folders_train.add(prefix)\n",
        "\n",
        "    for file_name in relevant_test_files:\n",
        "        prefix = re.findall(\"^(([^/]+/){4})\", file_name)[0]\n",
        "        sample_folders_test.add(prefix)\n",
        "    #Output a tuple (out_train,out_test) such that both are lists of tuples (label.json,log.json)\n",
        "    out_train = []\n",
        "    out_test  = []\n",
        "\n",
        "    for folder_name in sample_folders_train:\n",
        "        tuple_res   = [x for x in relevant_files if x.startswith(folder_name)]\n",
        "        label       = [x for x in tuple_res if x.endswith(\"label.json\")][0]\n",
        "        log         = [x for x in tuple_res if x.endswith(\"log.json\")][0]\n",
        "\n",
        "        out_train += [Sample(jsons_dict[label],jsons_dict[log])]\n",
        "\n",
        "    for folder_name in sample_folders_test:\n",
        "        tuple_res   = [x for x in relevant_files if x.startswith(folder_name)]\n",
        "        label       = [x for x in tuple_res if x.endswith(\"label.json\")][0]\n",
        "        log         = [x for x in tuple_res if x.endswith(\"log.json\")][0]\n",
        "\n",
        "        out_test += [Sample(jsons_dict[label],jsons_dict[log])]\n",
        "\n",
        "    return (out_train,out_test,ontology)\n",
        "\n",
        "out_train,out_test, ontology = load_data(home+\"Train_Test_Compressed.7z\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OxYU2p2p6pM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the gloVe word embeddings\n",
        "# Due to size restrictions these aren't in the project, but point to an external source; It is written such that all sizes can be used\n",
        "# GLOVE_LOCATION = r\"F:\\Temp\\NLP\\glove.6B.50d.txt\"\n",
        "GLOVE_LOCATION = r\"glove.6B.100d.txt\"\n",
        "\n",
        "with open(GLOVE_LOCATION,encoding=\"UTF-8\") as f:\n",
        "    lines = f.readlines()\n",
        "    lines = [x.split(\" \") for x in lines]\n",
        "    lines.sort()\n",
        "    glove_word_count = len(lines)\n",
        "    glove_vector_length = len(lines[0]) - 1\n",
        "    word_embedding = {x[0]:np.array(x[1:],dtype=float) for x in lines}\n",
        "    #Alternative as vector of words, and vector of vectors:\n",
        "    # words = [x[0] for x in lines]\n",
        "    # vecs = [np.array(x[1:]) for x in lines]\n",
        "    # mat = np.stack(vecs)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "CjQX2AiNC9MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A lot of words in Glove are kind of useless... there are for example a lot of numbers in there... see e.g words[:100]\n",
        "\n",
        "# The words in glvoe seem to be lower-text\n",
        "\n",
        "def word_to_vector(word_lowercased,embeddings):\n",
        "    exists = 1\n",
        "    try:\n",
        "        embedding = embeddings[word_lowercased]\n",
        "    except KeyError:\n",
        "        embedding = np.zeros((glove_vector_length))\n",
        "        exists = 0\n",
        "    return (exists,embedding)\n",
        "\n",
        "# Bisherig:\n",
        "# out_train,out_test, ontology = load_data(\"Train_Test_Compressed.7z\") ; tupel der trainings- und testdaten (als Klassen-Instanzen von Sample)\n",
        "#"
      ],
      "metadata": {
        "id": "vU1Gl62ZDWoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzMhKrNI6z14"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Input: A list of texts as texts, and in labels for each text whether the pragmatic act of the text is \"inform\". dim is the word-embedding dimension\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels,embeddings):\n",
        "        self.embeddings = embeddings\n",
        "        self.texts = []\n",
        "        self.labels = labels\n",
        "        self.dim = len(embeddings[list(embeddings.keys())[0]])\n",
        "        self.vectors = []\n",
        "        for index, text in enumerate(texts):\n",
        "            cur_vec = np.zeros((self.dim,),dtype=float)\n",
        "            existing_embeddings = 0\n",
        "            for word in text:\n",
        "                exists, add_vec = self.word_to_vector(word)\n",
        "                cur_vec += add_vec\n",
        "                existing_embeddings +=exists\n",
        "            self.texts += [cur_vec/existing_embeddings]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.texts[idx],self.labels[idx])\n",
        "\n",
        "\n",
        "    # The words in glvoe seem to be lower-text\n",
        "    def word_to_vector(self, word_lowercased):\n",
        "        exists = 1\n",
        "        try:\n",
        "            embedding = np.array(self.embeddings[word_lowercased],dtype=float)\n",
        "        except KeyError:\n",
        "            embedding = np.zeros((self.dim),dtype=float)\n",
        "            exists = 0\n",
        "        return (exists,embedding)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tests\n",
        "\n",
        "sentence_test1  = [\"this isn't a sentence\".split(\" \"), \"this is another sentence\".split(\" \")]\n",
        "embeddings_test1 = {\"this\":[0.1,0.2,0.3],\"is\":[0,1.,0.3]}\n",
        "\n",
        "TextDataset(sentence_test1,[1,1],embeddings_test1)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to-R1AGdIPGa",
        "outputId": "4f5da54c-ee53-4ac7-b7ae-3f7da652e0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.05, 0.6 , 0.3 ]), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_dXrlFa6-Vd"
      },
      "outputs": [],
      "source": [
        "# To do: set learning rate\n",
        "# To do: set number of training epochs\n",
        "learning_rate = 1e-3\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "ACT = 'inform'\n",
        "TRAIN_FOLDERS = ['Mar13_S0A0', 'Mar13_S0A1', 'Mar13_S1A0', 'Mar13_S1A1']\n",
        "TEST_FOLDERS = ['Mar13_S2A0', 'Mar13_S2A1']\n",
        "\n",
        "# To do: set batch size\n",
        "# To do: set dimension\n",
        "BATCH_SIZE = 16\n",
        "DIMENSION = 50\n",
        "HIDDEN_DIM = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJvU2ng37ASo"
      },
      "outputs": [],
      "source": [
        "embeddings = {}\n",
        "\n",
        "# To do: read from the GloVe vectors to create the embeddings dictionary \n",
        "# the dictionary maps each word to its representation\n",
        "with open('./glove.6B/glove.6B.' + str(DIMENSION) + 'd.txt') as handle:\n",
        "    for line in handle.readlines():\n",
        "        parts = line.strip('\\n').split(' ')\n",
        "        assert len(parts) == DIMENSION + 1\n",
        "        embeddings[parts[0]] = [float(i) for i in parts[1:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHeH5ZHT7CII"
      },
      "outputs": [],
      "source": [
        "\n",
        "        \n",
        "        \n",
        "def load_data(folder_list):\n",
        "    # To do: load data\n",
        "    # the data should be a list of lists of words\n",
        "    # the labels are a list of labels\n",
        "    \n",
        "    all_paths = []\n",
        "    for folder in folder_list:\n",
        "        all_paths.extend(['data/' + folder + '/' + i for i in os.listdir('data/' + folder)])\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "    for f in tqdm(all_paths):\n",
        "        if 'DS_Store' in f:\n",
        "            continue\n",
        "        with open(f + '/label.json') as handle:\n",
        "            label = json.loads(handle.read())\n",
        "        with open(f + '/log.json') as handle:\n",
        "            log = json.loads(handle.read())\n",
        "\n",
        "        assert len(label['turns']) == len(log['turns'])\n",
        "        for turn in range(len(label['turns'])):\n",
        "            turn_set = set()\n",
        "            for act in label['turns'][turn]['semantics']['json']:\n",
        "                turn_set.add(act['act'])\n",
        "            labels.append(1 if ACT in turn_set else 0)\n",
        "            data.append(log['turns'][turn]['input']['live']['asr-hyps'][0]['asr-hyp'].split(' '))\n",
        "\n",
        "        # print(data)\n",
        "        # print(labels)\n",
        "\n",
        "    return TextDataset(data, labels, embeddings, DIMENSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hTYI16VDeQa"
      },
      "outputs": [],
      "source": [
        "training_data = load_data(TRAIN_FOLDERS)\n",
        "test_data = load_data(TEST_FOLDERS)\n",
        "\n",
        "print('Creating dataloaders...')\n",
        "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miL-tKnH7GA8"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # To do: define your network\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(DIMENSION, HIDDEN_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_DIM, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To do: define the forward pass\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhcXHNNIdS9q"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwBNA-TZ7Hyx"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    # To do: write the training loop\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        # print(loss)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWRdUSG07Mgn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # To do: write the test loop\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrZN0DO7dVop"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# To do: define loss function\n",
        "# To do: define optimizer\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}